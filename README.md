# Neural Network Approximation using ReLU

This is a project for the **Scientific Computing Practical – Part II** at the University of Göttingen.

##  Goal
To approximate the function:
\[
f(x) = \sin(2\pi x)
\]
using a simple fully connected neural network built **from scratch** using NumPy and ReLU activation.

##  Contents

| File | Description |
|------|-------------|
| `01_intro_neuralnet.ipynb` | Main notebook with full explanation and plots |
| `approximate_relu.py` | Initial Python script for setup |
| `.gitignore` | Keeps repo clean by ignoring `venv/` and cache |

##  Example Output

The network learns to approximate the sine wave using forward & backward pass, trained over 5000 epochs.

![Example](https://via.placeholder.com/600x200.png?text=Insert+Graph+Screenshot+Here)

##  Concepts Covered
- ReLU activation
- Manual forward/backward pass
- MSE loss
- Weight updates
- Function approximation

##  Technologies
- Python
- NumPy
- Jupyter Notebook
- Git

##  Author
**Muhammad Luqman Ahmed**  
M.Sc. Mathematics – Data Science  
University of Göttingen

